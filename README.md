# Table of Contents:
- Introduction
- Dataset
- A/B Testing Process
- Data Storytelling
- Conclusion

# Introduction:
Online gaming agencies are constantly striving to improve user engagement and increase business outcomes. A/B testing is a powerful tool for making data-driven decisions and optimizing user engagement. In this case study, we will use a dataset from Kaggle to demonstrate the A/B testing process and show how it can be used to improve user engagement in online gaming agencies.

# Dataset:
The dataset we will be using is the "Video Game Sales with Ratings" dataset from Kaggle (https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings). This dataset contains information about video games, including sales data and user ratings. We will use this dataset to perform A/B testing and compare the effectiveness of two different designs for engaging users.

# A/B Testing Process:
The A/B testing process involves several steps:

1. Define the hypothesis: We will define the null hypothesis and alternative hypothesis to test the effectiveness of two different designs for engaging users.
2. Split the sample: We will randomly split the dataset into two samples, one for each design.
3. Collect data: We will collect data on user engagement for each design.
4. Analyze the data: We will calculate the mean and standard deviation for each sample and perform a hypothesis test to determine if there is a significant difference between the two designs.
5. Draw conclusions: We will use the results of the hypothesis test to draw conclusions about which design is more effective for engaging users.

**READ THE FULL ARTICLE ON [MEDIUM](https://medium.com/@yennhi95zz/optimizing-user-engagement-in-online-gaming-agency-with-a-b-testing-a-case-study-566777d22a3d)**
